{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Based Reinforcement Learning Hopper Environment\n",
    "\n",
    "In this notebook, we implement a set-based reinforcement learning algorithm, which is based on the paper [Training Verifiably Robust Agents using Set-Based Reinforcement Learning](https://arxiv.org/abs/2408.09112). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from SBML import ZonoTorch as zt\n",
    "from SBML import SBRL as sbrl\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seed = 4\n",
    "\n",
    "seedtorch = torch.random.manual_seed(seed)\n",
    "seednp = np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor and Critic models are implemented using PyTorch. The actor and critic are simple feedforward neural networks with 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = torch.nn.Sequential(\n",
    "    torch.nn.Linear(11, 400),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(400, 300),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(300, 3),\n",
    "    torch.nn.Tanh()\n",
    ")\n",
    "\n",
    "critic = torch.nn.Sequential(\n",
    "    torch.nn.Linear(14, 400),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(400, 300),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(300, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel/ETH_Code/SemProj/SetBasedRL/.venv/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/manuel/ETH_Code/SemProj/SetBasedRL/.venv/lib/python3.8/site-packages/gym/envs/mujoco/mujoco_env.py:190: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env_options = {\n",
    "    'max_step': 1000,\n",
    "}\n",
    "\n",
    "senv = sbrl.GymEnvironment('Hopper-v3',env_options, DEVICE)\n",
    "\n",
    "ddpg_ops = {\n",
    "    'actor_lr': 1e-4,\n",
    "    'actor_train_mode': 'set',\n",
    "    'critic_lr': 1e-3,\n",
    "    'critic_l2': 0.01,\n",
    "    'critic_train_mode': 'point',\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.001,\n",
    "    'buffer_size': 1e6,\n",
    "    'batch_size': 64,\n",
    "    'exp_noise': 0.2,\n",
    "    'action_ub': 1,\n",
    "    'action_lb': -1,\n",
    "    'noise': .1,\n",
    "    'actor_eta': 0.001,\n",
    "    'actor_omega': 0.5,\n",
    "}\n",
    "agent = sbrl.DDPG(actor,critic,ddpg_ops,DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcment Learning Parameters:\n",
      "=================================\n",
      "Standard-RL Options:\n",
      "--------------------\n",
      "Discount Factor (gamma): 0.99\n",
      "Buffer Size: 1000000.0\n",
      "Batch Size: 64\n",
      "Steps: 3000000.0\n",
      "Device: cuda\n",
      "\n",
      "Actor Options:\n",
      "--------------\n",
      "Learning Rate: 0.0001\n",
      "Training Mode: set\n",
      "Eta: 0.001\n",
      "Omega: 0.5\n",
      "Noise: 0.1\n",
      "\n",
      "Critic Options:\n",
      "---------------\n",
      "Learning Rate: 0.001\n",
      "Training Mode: point\n",
      "=================================\n",
      "\n",
      "\n",
      "Found 4 GPUs for rendering. Using device 0.\n",
      "Training Information:\n",
      "=====================\n",
      "|Step           |Time   |Reward         |Q-Value        |Critic-Loss    |Actor-Loss     |\n",
      "|---------------|-------|---------------|---------------|---------------|---------------|\n",
      "|1.00e+00\t|0.0\t|1.00e+00\t|7.15e-02\t|0.00e+00\t|0.00e+00\t|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel/ETH_Code/SemProj/SetBasedRL/.venv/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|1.00e+03\t|0.3\t|6.18e+00\t|2.07e+00\t|2.86e-02\t|-2.09e+00\t|\n",
      "|2.00e+03\t|0.5\t|3.10e+00\t|2.99e+00\t|2.97e-01\t|-4.04e+00\t|\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msenv\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3e6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ETH_Code/SemProj/SetBasedRL/examples/../SBML/SBRL/actorcitic.py:139\u001b[0m, in \u001b[0;36mActorCritic.train\u001b[0;34m(self, env, steps, verbose)\u001b[0m\n\u001b[1;32m    136\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mfull \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mindx \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 139\u001b[0m     critic_loss, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     total_actor_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m actor_loss\n\u001b[1;32m    141\u001b[0m     total_critic_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m critic_loss\n",
      "File \u001b[0;32m~/ETH_Code/SemProj/SetBasedRL/examples/../SBML/SBRL/algorithms/ddpg.py:135\u001b[0m, in \u001b[0;36mDDPG.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_critic(states,actions,target)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor_train_mode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 135\u001b[0m     z_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugmentState\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    136\u001b[0m     eval_states \u001b[38;5;241m=\u001b[39m Zonotope(z_states)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/ETH_Code/SemProj/SetBasedRL/examples/../SBML/SBRL/actorcitic.py:265\u001b[0m, in \u001b[0;36mActorCritic.augmentState\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m state\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    264\u001b[0m noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 265\u001b[0m noise_batch \u001b[38;5;241m=\u001b[39m \u001b[43mnoise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([state\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m),noise_batch],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(senv,3e6,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract reward history\n",
    "rewards = np.array(agent.learn_hist['reward'])\n",
    "\n",
    "# Compute moving average with a window of 10\n",
    "window_size = 10\n",
    "moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "steps_executed = 1000 * np.arange(len(rewards))\n",
    "\n",
    "# Plot the reward history\n",
    "plt.figure()\n",
    "plt.plot(steps_executed,rewards, label='Raw Reward', alpha=0.3)\n",
    "plt.plot(1000 *np.arange(window_size - 1, len(rewards)), moving_avg, label='Moving Average (10 evals)', color='red')\n",
    "plt.title('Reward')\n",
    "plt.legend()\n",
    "\n",
    "if not os.path.exists('figures'):\n",
    "    os.makedirs('figures')\n",
    "\n",
    "plt.savefig('figures/LearnHist.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model:\n",
    "torch.save(agent.actor.to('cpu').state_dict(), 'actor.pth')\n",
    "torch.save(agent.critic.to('cpu').state_dict(), 'critic.pth')\n",
    "\n",
    "# Save the reward history\n",
    "np.save('rewards.npy', rewards)\n",
    "np.save('reward_mvg.npy', moving_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
